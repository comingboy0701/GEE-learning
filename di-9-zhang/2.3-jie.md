# 第2.3节 经典机器学习-其它机器学习及工具


## 1 SVM 


$Ax+By+Cz+D=0$

点到面的距离公式：$\frac{|Ax+By+Cz+D|}{\sqrt{A^2+B^2+C^2}}$
 
(1)$y_{i} = 1$: $wx_{i}+b>=1$
 
(2)$y_{i} = -1$: $wx_{i}+b<=-1$

(3) 1,2 公式合并： $y_{i}(wx_{i}+b)>=1$

两个面的距离： $\frac{2}{||w^2||}$

合并上面两个不等式：

$\left[\begin{array}{ll}
min\frac{1}{2}||\theta^{2}||  \\
y_{i}(\theta x_{i})>=1 
\end{array}\right.$

拉格朗日乘子法进行求解：


## 2 kmeans 

problems:

1. 初始化中心点

- 随机选择一个中心点
- 再选一个中心点根据概率：$\frac{D(x)^{2}}{\sum_{x \in \mathcal{X}} D(x)^{2}}$
- 重复上面一步
2. K 选择

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import cv2

def assignment(df, centroids, colmap):
    for i in centroids.keys():
        # sqrt((x1 - x2)^2 - (y1 - y2)^2)
        df['distance_from_{}'.format(i)] = (
            np.sqrt(
                (df['x'] - centroids[i][0]) ** 2
                + (df['y'] - centroids[i][1]) ** 2
            )
        )
    distance_from_centroid_id = ['distance_from_{}'.format(i) for i in centroids.keys()]
    df['closest'] = df.loc[:, distance_from_centroid_id].idxmin(axis=1)
    df['closest'] = df['closest'].map(lambda x: int(x.lstrip('distance_from_')))
    df['color'] = df['closest'].map(lambda x: colmap[x])
    return df

def update(df, centroids):
    for i in centroids.keys():
        centroids[i][0] = np.mean(df[df['closest'] == i]['x'])
        centroids[i][1] = np.mean(df[df['closest'] == i]['y'])
    return centroids

df = pd.DataFrame({
        'x': [12, 20, 28, 18, 10, 29, 33, 24, 45, 45, 52, 51, 52, 55, 53, 55, 61, 64, 69, 72, 23],
        'y': [39, 36, 30, 52, 54, 20, 46, 55, 59, 63, 70, 66, 63, 58, 23, 14, 8, 19, 7, 24, 77]
    })


k = 3
# centroids[i] = [x, y]
centroids = {
    i: [np.random.randint(0, 80), np.random.randint(0, 80)]
    for i in range(k)
}

# step 0.2: assign centroid for each source data
# for color and mode: https://blog.csdn.net/m0_38103546/article/details/79801487
# colmap = {0: 'r', 1: 'g', 2: 'b', 3: 'm', 4: 'c'}
colmap = {0: 'r', 1: 'g', 2: 'b'}
df = assignment(df, centroids, colmap)

plt.subplot(1,2,1)
plt.scatter(df['x'], df['y'], color=df['color'], alpha=0.5, edgecolor='k')
for i in centroids.keys():
    plt.scatter(*centroids[i], color=colmap[i], linewidths=6)
plt.xlim(0, 80)
plt.ylim(0, 80)

for i in range(10):
    centroids = update(df, centroids)
    df = assignment(df, centroids, colmap)

    
plt.subplot(1,2,2)
plt.scatter(df['x'], df['y'], color=df['color'], alpha=0.5, edgecolor='k')
for i in centroids.keys():
    plt.scatter(*centroids[i], color=colmap[i], linewidths=6)
plt.xlim(0, 80)
plt.ylim(0, 80)
plt.show()
```

<!-- #region -->
## 3 过拟合和欠拟合


|  | Underdefit | Overfit |
| --- | --- | --- |
| Phenomenon | 1. Not good even when training  | 1. Very good when training, bad when validating |
| Reasons | 1. Models too sampe, <br> 2. Data's too complex,<br>3.Gradient/Weight's too small,<br>4. Parameter's too less,<br> 5. Too much regularization<br> | 1. Model's too complex, <br>2. Data's too smaple, <br>3. Gradient/Weight's too big, <br>4. Parmeter's too many,<br> 5. Less regularization<br> |
| Solutons |1. More complex structure, <br>2. Less data, <br>3. Simple data,<br> 4. Less regularization <br> | 1. simpler structure,<br> 2. More data, <br>3. Complex data, <br>4. More regularization,<br> 5. Dropout,<br> 6. Batch Norm,<br> 7. Preturb Label, <br>8. Noise,<br> 9. Early stop,<br>10. .... |<br>
<!-- #endregion -->

<!-- #region -->
## 4 偏差（bias）和方差(variance)

1. Bias是用所有可能的训练数据集训练出的所有模型的输出的平均值与真实模型的输出值之间的差异。


2. Variance是不同的训练数据集训练出的模型输出值之间的差异
<!-- #endregion -->

## 5 梯度消失和梯度爆炸
