# 第2.2节 经典机器学习-神经网络、反向传播算法以及正则化


## 1 监督学习

激活函数：加入非线性关系


## 2 反向传播


## 3 前向传播


## 4 链式传播的一些问题

梯度消失：只用sigmod函数，一般不超过三层


## 5 正则化


**Linear Regression:**

$J(\theta)=\frac{1}{2 m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{i}\right)-y^{i}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}\right]$

Gradient Descent:

$\theta_{j}=\theta_{j}-\alpha\left[\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{i}\right)-y^{i}\right) \cdot x^{i}+\frac{\lambda}{m} \theta_{j}\right]$

$\theta_{j} = \theta_{j}(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum\left(h_{\theta}\left(x^i\right)-y^i\right)\cdot x^i$

**Logistic Regression:**

$J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{i} \log \left(h_{\theta}\left(x^{i}\right)\right)+\left(1-y^{i}\right) \log \left(1-h_{\theta}\left(x^{i}\right)\right)\right]+\frac{\lambda}{2 m} \sum_{j=1}^{n} \theta_{j}^{2}$



**L1** 和 **L2** 正则化

Lp Norm: $\left(\sum|\theta_{i}^p|\right)^{\frac{1}{p}}$

L1 regularzation：$||\theta||^2$   **Ridge Regression**

计算效率快

结果不是稀疏矩阵

不能做特征选择

L2 regularzation:$||\theta||$  **Lasso Regression**

计算效率慢

产生稀疏矩阵的结果

可以做特征选择
